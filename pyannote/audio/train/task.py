#!/usr/bin/env python
# encoding: utf-8

# The MIT License (MIT)

# Copyright (c) 2019 CNRS

# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:

# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.

# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

# AUTHORS
# HervÃ© BREDIN - http://herve.niderb.fr

"""
Tasks
#####

This module provides a `Task` class meant to specify machine learning tasks
(e.g. classification or regression).

This may be used to infer parts of the network architecture and the associated
loss function automatically.

Example
-------
>>> voice_activity_detection = Task(type=TaskType.MULTI_CLASS_CLASSIFICATION,
...                                 output=TaskOutput.SEQUENCE)
"""


from typing import List, Dict, Text, Type, Union, Tuple
from pathlib import Path

from enum import Enum
import multiprocessing

from pyannote.database import Protocol
from pyannote.database import ProtocolFile
from pyannote.database import Preprocessors
from pyannote.database import Subset
from pyannote.database.custom import gather_loaders

from torch.utils.data import DataLoader
from torch.utils.data import IterableDataset

import pytorch_lightning as pl
import torch
import torch.nn as nn
import torch.nn.functional as F

import yaml
from argparse import Namespace

import math
import numpy as np
from pyannote.core.utils.helper import get_class_by_name
from pyannote.core import Segment

from tqdm import tqdm


class Resolution(Enum):
    FRAME = "frame"
    CHUNK = "chunk"


class Problem(Enum):
    """Type of machine learning problem

    Used to automatically suggest reasonable default final activation layer
    and loss function.
    """

    MULTI_CLASS_CLASSIFICATION = "classification"
    MULTI_LABEL_CLASSIFICATION = "multi-label classification"
    REGRESSION = "regression"
    REPRESENTATION = "representation"


class BaseTask(pl.LightningModule):
    @staticmethod
    def load_config(
        config_yml: Path, hparams_yml: Path = None
    ) -> Tuple[Type["BaseTask"], Namespace, Preprocessors]:
        """Load and parse configuration file

        Parameters
        ----------
        config_yml : Path
            Path to configuration file
        hparams_yml : Path
            Path to Pytorch-lightning hyper-parameter file

        Returns
        -------
        task_class : type
            Task class (e.g. SpeechActivityDetection)
        hparams : Namespace
            Hyper-parameters.
        preprocessors : Preprocessors
            Preprocessors.
        """

        with open(config_yml, "r") as fp:
            configuration = yaml.load(fp, Loader=yaml.SafeLoader)

        task_section = configuration.pop("task")
        task_class = get_class_by_name(task_section["name"])

        if hparams_yml is not None:
            with open(hparams_yml, "r") as fp:
                configuration = yaml.load(fp, Loader=yaml.SafeLoader)

        for key, value in task_section["params"].items():
            configuration[key] = value

        # preprocessors
        preprocessors_section = configuration.pop("preprocessors", dict())
        preprocessors, loaders = dict(), dict()
        for key, preprocessor in preprocessors_section.items():
            #    key:
            #       name: package.module.ClassName
            #       params:
            #          param1: value1
            #          param2: value2
            if isinstance(preprocessor, dict):
                preprocessor_class = get_class_by_name(preprocessor["name"])
                preprocessors[key] = preprocessor_class(
                    **preprocessor.get("params", dict())
                )
            #    key: /path/to/file.suffix
            else:
                loaders[key] = preprocessor
        preprocessors.update(gather_loaders(loaders))

        hparams = Namespace(**configuration)
        return task_class, hparams, preprocessors

    def __init__(
        self,
        hparams: Union[Namespace, Dict],
        protocol: Protocol = None,
        subset: Subset = "train",
        files: List[ProtocolFile] = None,
        training: bool = True,
        num_workers: int = None,
    ):
        super().__init__()

        if isinstance(hparams, dict):
            hparams = Namespace(**hparams)

        self.hparams = hparams
        if num_workers is None:
            num_workers = multiprocessing.cpu_count()
        self.num_workers = num_workers

        # AUGMENTATION
        if training and "data_augmentation" in self.hparams:
            AugmentationClass = get_class_by_name(
                self.hparams.data_augmentation["name"]
            )
            augmentation = AugmentationClass(**self.hparams.data_augmentation["params"])

        else:
            augmentation = None

        # FEATURE EXTRACTION
        FeatureExtractionClass = get_class_by_name(
            self.hparams.feature_extraction["name"]
        )
        self.feature_extraction = FeatureExtractionClass(
            **self.hparams.feature_extraction["params"], augmentation=augmentation
        )

        # TRAINING DATA
        if training:
            if files is not None:
                self.files = files
            elif protocol is not None:
                # load files lazily once to know their number
                files = list(getattr(protocol, subset)())
                # really load files and show a progress bar
                for f in tqdm(
                    iterable=files, desc="Loading training metadata", unit="file"
                ):
                    _ = dict(f)
                self.files = files
                # self.files = list(dict(f) for f in getattr(protocol, subset)())
            else:
                msg = "No training protocol available. Please provide one."
                raise ValueError(msg)

        # MODEL
        ArchitectureClass = get_class_by_name(self.hparams.architecture["name"])
        architecture_params = self.hparams.architecture.get("params", dict())
        self.model = ArchitectureClass(self, **architecture_params)

        if training:
            self.prepare_metadata()

        # EXAMPLE INPUT ARRAY (used by Pytorch Lightning to display in and out
        # sizes of each layer, for a batch of size 5)
        if "duration" in self.hparams:
            duration = self.hparams.duration
            context = self.feature_extraction.get_context_duration()
            num_samples = math.ceil(
                (2 * context + duration) * self.feature_extraction.sample_rate
            )
            waveform = np.random.randn(num_samples, 1)
            self.example_input_array = torch.unsqueeze(
                torch.tensor(
                    self.feature_extraction.crop(
                        {"waveform": waveform, "duration": 2 * context + duration},
                        Segment(context, context + duration),
                        mode="center",
                        fixed=duration,
                    ),
                ),
                0,
            ).repeat(5, 1, 1)

    def prepare_metadata(self):
        pass

    def prepare_data(self):
        pass

    def train_dataset(self) -> IterableDataset:
        raise NotImplementedError("")

    def train_dataloader(self) -> DataLoader:
        return DataLoader(
            self.train_dataset(),
            batch_size=self.hparams.batch_size,
            num_workers=self.num_workers,
        )

    # =========================================================================
    # CLASSES
    # =========================================================================

    @property
    def classes(self) -> List[Text]:
        """List of classes

        Used to automatically infer the output dimension of the model
        """
        if "classes" not in self.hparams:
            self.hparams.classes = self.get_classes()
        return self.hparams.classes

    def get_classes(self) -> List[Text]:
        """Compute list of classes

        Called when classes depend on the training data (e.g. for domain
        classification experiments where we do not know in advance what
        domains are)
        """
        msg = f"Class {self.__class__.__name__} must define a 'get_classes' method."
        raise NotImplementedError(msg)

    # =========================================================================
    # LOSS FUNCTION
    # =========================================================================

    def guess_activation(self):

        if self.problem == Problem.MULTI_CLASS_CLASSIFICATION:
            return nn.LogSoftmax(dim=-1)

        elif self.problem == Problem.MULTI_LABEL_CLASSIFICATION:
            return nn.Sigmoid()

        elif self.problem == Problem.REGRESSION:
            return nn.Identity()

        elif self.problem == Problem.REPRESENTATION:
            return nn.Identity()

        else:
            msg = f"Unknown default activation for '{self.problem}' problems."
            raise NotImplementedError(msg)

    def get_activation(self):
        return self.guess_activation()

    def guess_loss(self):

        if (
            self.problem == Problem.MULTI_CLASS_CLASSIFICATION
            and self.resolution_output == Resolution.FRAME
        ):

            def loss(
                y_pred: torch.Tensor, y: torch.Tensor, weight=None
            ) -> torch.Tensor:
                return F.nll_loss(
                    y_pred.view((-1, len(self.classes))),
                    y.view((-1,)),
                    weight=weight,
                    reduction="mean",
                )

            return loss

        if (
            self.problem == Problem.MULTI_CLASS_CLASSIFICATION
            and self.resolution_output == Resolution.CHUNK
        ):

            def loss(
                y_pred: torch.Tensor, y: torch.Tensor, weight=None
            ) -> torch.Tensor:
                return F.nll_loss(y_pred, y, weight=weight, reduction="mean",)

            return loss

        msg = (
            f"Cannot guess loss function for {self.__class__.__name__}. "
            f"Please implement {self.__class__.__name__}.get_loss method."
        )
        raise NotImplementedError(msg)

    def get_loss(self):
        return self.guess_loss()

    @property
    def loss(self):
        if not hasattr(self, "loss_"):
            self.loss_ = self.get_loss()
        return self.loss_

    # =========================================================================
    # TRAINING LOOP
    # =========================================================================

    def configure_optimizers(self):

        OptimizerClass = get_class_by_name(self.hparams.optimizer["name"])
        optimizer_params = self.hparams.optimizer.get("params", dict())
        optimizer = OptimizerClass(
            self.parameters(), lr=self.hparams.learning_rate, **optimizer_params,
        )

        return optimizer

    def forward(self, chunks: torch.Tensor) -> torch.Tensor:
        return self.model(chunks)

    def training_step(self, batch, batch_idx):
        X = batch["X"]
        y = batch["y"]
        y_pred = self(X)
        loss = self.loss(y_pred, y)
        logs = {"loss": loss}
        return {"loss": loss, "log": logs}
